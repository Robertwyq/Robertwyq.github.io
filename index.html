<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Yuqi Wang</title>
  
  <meta name="author" content="Yuqi Wang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>

<body>
  <table style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yuqi Wang (Robert)</name>
              </p>
              <p>
              I am currently a researcher at <a href="https://www.bytedance.com/">Bytedance</a>. I received my Ph.D. degree in the NLPR, Institute of Automation, Chinese Academy of Sciences (CASIA), supervised by Prof. <a href="https://zhaoxiangzhang.net" target="_blank">Zhaoxiang Zhang</a>. Prior to that, I obtained my Bachelor's degree in Automation (Robotics) from the College of Control Science and Engineering at Zhejiang University (ZJU) in 2020.
              Additionally, I interned at <a href="https://www.meituan.com/">Meituan</a>, under the supervision of Fei Xia, and at <a href="https://www.baai.ac.cn/">BAAI</a>, where I was mentored by Dr. <a href="https://www.xloong.wang/" target="_blank">Xinlong Wang</a>.
              </p>  
              <p>
              My overarching research passion lies in 
              <strong>embodied understanding and planning in open-world environments</strong>, 
              with a particular focus on exploring <strong>world models</strong> and their applications in the physical world.
              My research interests span 
              <strong>computer vision</strong>, 
              <strong>unsupervised learning</strong>, 
              <strong>3D perception</strong>, 
              <strong>world models</strong>, and 
              <strong>video generation</strong>, 
              all aiming toward comprehensive open-world 3D scene perception and understanding.
              </p>
              <p style="text-align:center">
                <a href="mailto:wangyuqi2020@ia.ac.cn">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=35UcX9sAAAAJ"> Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/Robertwyq"> Github </a> &nbsp/&nbsp
                <a href="data/CV_Yuqi_Wang.pdf">Curriculum Vitae</a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <img style="width:80%;max-width:80%" alt="profile photo" src="img/wyq.jpeg">
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p>
              <li style="margin: 5px;" >
                <b>2025-06:</b> <strong>Two</strong> papers are accepted to <a href="https://iccv.thecvf.com/">ICCV 2025</a>.
              <li style="margin: 5px;" >
                <b>2025-01:</b> <strong>Two</strong> papers are accepted to <a href="https://iclr.cc/">ICLR 2025</a>.
              <li style="margin: 5px;" >
                <b>2024-09:</b> <strong>One</strong> paper on driving world model is accepted to <a href="https://neurips.cc/">NeurIPS 2024 Dataset Track</a>.
              <li style="margin: 5px;" >
                <b>2024-07:</b> <strong>One</strong> paper on indoor monocular occupancy is accepted to <a href="https://eccv.ecva.net/">ECCV 2024</a>.
              <li style="margin: 5px;" >
                <b>2024-02:</b> <strong>Two</strong> papers on driving world model and occupancy prediction is accepted to <a href="https://cvpr.thecvf.com/">CVPR 2024</a>.
              <li style="margin: 5px;" >
                <b>2023-12:</b> <strong>One</strong> paper on multi-agent representation learning is accepted to <a href="https://academic.oup.com/nsr">Nation Science Review</a>.
              <li style="margin: 5px;" >
                <b>2023-07:</b> <strong>One</strong> paper on unsupervised instance segmentation is accepted to <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">TPAMI</a>.
              <li style="margin: 5px;" >
                <b>2023-02:</b> <strong>One</strong> paper on 3D object perception is accepted to <a href="http://cvpr2023.thecvf.com/">CVPR 2023</a>.
              <li style="margin: 5px;" >
                <b>2022-09:</b> <strong>One</strong> paper on unsupervised object discovery is accepted to <a href="https://neurips.cc/">NeurIPS 2022</a>.
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                * indicates equal contribution
              </p>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="img/univla.png" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>Unified Vision-Language-Action Model</papertitle>
                <br>
                <strong>Yuqi Wang</strong>, <a>Xinghang Li</a>, <a>Wenxuan Wang</a>, <a>Junbo Zhang</a>, <a>Yingyan Li</a>, <a>Yuntao Chen</a>, <a>Xinlong Wang</a>, <a>Zhaoxiang Zhang</a>
                <br>
                <em>arXiv, 2025
                <br>
                <a href="https://arxiv.org/abs/2506.19850">[paper]</a> <a href="https://robertwyq.github.io/univla.github.io">[Page]</a> <a href="https://github.com/baaivision/UniVLA">[Code]</a>
                <br>
                <p> unified vision-language-action model for embodied intelligence </p>
            </td>
            </tr>
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="img/drivinggpt.png" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>DrivingGPT: Unifying Driving World Modeling and Planning with Multi-modal Autoregressive Transformers</papertitle>
                <br>
                <a>Yuntao Chen</a>, <strong>Yuqi Wang</strong>, <a>Zhaoxiang Zhang</a>
                <br>
                <em>ICCV, 2025
                <br>
                <a href="https://arxiv.org/abs/2412.18607">[paper]</a> <a href="https://rogerchern.github.io/DrivingGPT">[Page]</a>
                <br>
                <p> Unifying world model and planning in autonomous driving</p>
            </td>
            </tr>
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="img/WoTE.jpg" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>End-to-End Driving with Online Trajectory Evaluation via BEV World Model</papertitle>
                <br>
                <a>Yingyan Li*</a>, <strong>Yuqi Wang*</strong>, <a>Yang Liu</a>, <a>Jiawei He</a>, <a>Lue Fan</a>, <a>Zhaoxiang Zhang</a>
                <br>
                <em>ICCV, 2025
                <br>
                <a href="https://arxiv.org/abs/2504.01941">[paper]</a> <a href="https://github.com/liyingyanUCAS/WoTE">[Code]</a>
                <br>
                <p> An end-to-end autonomous driving framework that leverages a BEV-based world model to predict future agent states, enabling online trajectory evaluation and selection. </p>
            </td>
            </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="img/freevs.png" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>FreeVS: Generative View Synthesis on Free Driving Trajectory</papertitle>
                <br>
                <a>Qitai Wang</a>, <a>Lue Fan</a>, <strong> Yuqi Wang</strong>, <a>Yuntao Chen</a>, <a>Zhaoxiang Zhang</a>
                <br>
                <em>ICLR, 2025
                <br>
                <a href="https://arxiv.org/abs/2410.18079">[paper]</a> <a href="https://freevs24.github.io/">[Page]</a> <a href="https://github.com/esdolo/FreeVS">[Code]</a>
                <br>
                <p> Generative view synthesis on free driving trajectory </p>
            </td>
            </tr>
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="img/law.png" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>Enhancing End-to-End Autonomous Driving with Latent World Model</papertitle>
                <br>
                <a>Yingyan Li</a>, <a>Lue Fan</a>,<a>Jiawei He</a>, <strong> Yuqi Wang</strong>, <a>Yuntao Chen</a>, <a>Zhaoxiang Zhang</a>
                <br>
                <em>ICLR, 2025
                <br>
                <a href="https://arxiv.org/pdf/2406.08481">[paper]</a> <a href="https://github.com/BraveGroup/LAW">[Code]</a>
                <br>
                <p> Latent world model as a self-supervised learning proxy for end-to-end autonomous driving </p>
            </td>
            </tr>

        <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="img/drivingdojo.png" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>DrivingDojo Dataset: Advancing Interactive and Knowledge-Enriched Driving World Model</papertitle>
                <br>
                <strong> Yuqi Wang</strong>*, <a>Ke Cheng</a>*, <a>Jiawei He</a>*, <a>Qitai Wang</a>*, <a>Hengchen Dai</a>, <a>Yuntao Chen</a>, <a>Fei Xia</a>, <a>Zhaoxiang Zhang</a>
                <br>
                <em>NeurIPS, 2024, D&B Track
                <br>
                <a href="https://arxiv.org/pdf/2410.10738v1">[paper]</a> <a href="https://drivingdojo.github.io/">[Page]</a> <a href="https://github.com/Robertwyq/Drivingdojo">[code]</a>
                <br>
                <p> DrivingDojo dataset  features video clips with a complete set of driving maneuvers, diverse multi-agent interplay, and rich open-world driving knowledge, laying a stepping stone for future world model development. </p>
            </td>
            </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="img/worldmodel_survey.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond</papertitle>
              <br>
              <a> Zheng Zhu*</a>, 
              <a> Xiaofeng Wang*</a>,
              <a> Wangbo Zhao*</a>, 
              <a >Chen Min*</a>, 
              <a >Nianchen Deng*</a>, 
              <a >Min Dou*</a>, 
              <strong>Yuqi Wang*</strong>, 
              <a >Botian Shi</a>,
              <a >Kai Wang</a>,
              <a >Chi Zhang</a>,
              <a >Yang You</a>,
              <a >Zhaoxiang Zhang</a>,
              <a>Dawei Zhao</a>,
              <a >Liang Xiao</a>,
              <a >Jian Zhao</a>,
              <a >Jiwen Lu</a>,
              <a >Guan Huang</a>
              <br>
              <em>arXiv, 2024
              <br>
              <a href="https://arxiv.org/abs/2405.03520">[paper]</a> <a href="https://github.com/GigaAI-research/General-World-Models-Survey">[code]</a>
              <br>
              <p> A comprehensive survey on general world models, including world models for video generation, autonomous driving and autonomous agents. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="img/ISO.gif" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Monocular Occupancy Prediction for Scalable Indoor Scenes</papertitle>
              <br>
             <a>Hongxiao Yu</a>, <strong>Yuqi Wang</strong>, <a>Yuntao Chen</a>, <a>Zhaoxiang Zhang</a>
              <br>
              <em>ECCV, 2024
              <br>
              <a href="https://arxiv.org/abs/2407.11730">[paper]</a> <a href="https://hongxiaoy.github.io/ISO/">[page]</a> <a href="https://github.com/hongxiaoy/ISO">[code]</a>
              <br>
              <p> ISO, a method for monocular occupancy prediction in indoor scenes. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="img/drive-wm/logo.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Driving into the Future: Multiview Visual Forecasting and Planning with World Model for Autonomous Driving</papertitle>
              <br>
              <strong>Yuqi Wang*</strong>, <a>Jiawei He*</a>, <a>Lue Fan*</a>, <a>Hongxin Li*</a>, <a>Yuntao Chen</a>, <a>Zhaoxiang Zhang</a>
              <br>
              <em>CVPR, 2024
              <br>
              <a href="https://arxiv.org/abs/2311.17918.pdf">[paper]</a> <a href="https://drive-wm.github.io/">[Page]</a>  <a href="https://github.com/BraveGroup/Drive-WM">[code]</a>
              <br>
              <p> Drive-WM, a pioneering multi-view world model for end-to-end autonomous driving. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="img/panoocc/pipeline.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>PanoOcc: Unified Occupancy Representation for Camera-based 3D Panoptic Segmentation</papertitle>
              <br>
                <strong>Yuqi Wang</strong>, <a>Yuntao Chen</a>, <a>Xingyu Liao</a>, <a>Lue Fan</a>, <a>Zhaoxiang Zhang</a>
              <br>
              <em>CVPR, 2024
              <br>
              <a href="https://arxiv.org/abs/2306.10013">[paper]</a> <a href="https://github.com/Robertwyq/PanoOcc">[code]</a>
              <br>
              <p> PanoOcc, a method for camera-based 3D panoptic scene understanding.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="img/machinelanguage/ml.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Emergence of Machine Language: Towards Symbolic Intelligence with Neural Networks</papertitle>
              <br>
              <strong>Yuqi Wang</strong>, <a>Xu-Yao Zhang</a>, <a>Cheng-Lin Liu</a>, <a>Tieniu Tan</a>, <a>Zhaoxiang Zhang</a>
              <br>
              <em>National Science Review (NSR), 2024
              <br>
              <a href="https://doi.org/10.1093/nsr/nwad317">[paper]</a> 
              <br>
              <p> Emergence of machine language.</p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="img/affinity/pipeline.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Object Affinity Learning: Towards Annotation-Free Instance Segmentation</papertitle>
              <br>
              <strong>Yuqi Wang</strong>, <a>Yuntao Chen</a>, <a>Zhaoxiang Zhang</a>
                <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2024
              <br>
              <a href="https://ieeexplore.ieee.org/abstract/document/10192375">[paper]</a> <a href="https://github.com/Robertwyq/Object-Affinity">[Code]</a>
              <br>
              <p> 2D object discovery through depth and flow cues. </p>
            </td>
          </tr>

          <tr></tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="img/frustum/frustum.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>FrustumFormer: Adaptive Instance-aware Resampling for Multi-view 3D Detection</papertitle>
              <br>
             <strong>Yuqi Wang</strong>, <a>Yuntao Chen</a>, <a>Zhaoxiang Zhang</a>
              <br>
              <em>CVPR, 2023
              <br>
              <a href="https://arxiv.org/abs/2301.04467">[paper]</a> <a href="https://github.com/Robertwyq/Frustum">[Code]</a> <a href="https://www.bilibili.com/video/BV1mM4y1i7vd/?spm_id_from=333.999.0.0">[Bilibili]</a>
              <br>
              <p> FrustumFormer, enhancing vision-based 3D object detection through 2D prior. </p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="img/lsmol/lsmol_pipeline.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>4D Unsupervised Object Discovery</papertitle>
              <br>
             <strong>Yuqi Wang</strong>, <a>Yuntao Chen</a>, <a>Zhaoxiang Zhang</a>
              <br>
              <em>NeurIPS, 2022, <font color="red"><strong>(Spotlight)</strong></font>
              <br>
              <a href="https://arxiv.org/pdf/2210.04801.pdf">[paper]</a> <a href="https://github.com/Robertwyq/LSMOL">[Code]</a>
              <br>
              <p> 4D unsupervised object discovery using camera and LiDAR raw information. </p>
            </td>
          </tr>
        

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Honors and Awards</heading>
              <p>
                <li style="margin: 5px;"> 2025 中科院院长奖 </li>
                <li style="margin: 5px;"> 2025 北京市优秀毕业生 </li>
                <li style="margin: 5px;"> 2024 National Scholarship / 国家奖学金 </li>
                <li style="margin: 5px;"> 2023 朱李月华奖学金 </li>
                <li style="margin: 5px;"> 2020 浙江省优秀毕业生 </li>
                <li style="margin: 5px;"> 2018 National Scholarship / 国家奖学金 </li>
                <li style="margin: 5px;"> 2018 中控奖学金 </li>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Website Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
  
<p><center>
	  <div id="clustrmaps-widget" style="width:5%">
    <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=pYUe-9vNee5nJl9ztd0lgo-xYiNaKRYwhjvT3xnX5Mg"></script>
	  </div>        
	  <br>
	    &copy; Yuqi Wang | Last updated: July 12, 2025
</center></p>
</body>

</html>